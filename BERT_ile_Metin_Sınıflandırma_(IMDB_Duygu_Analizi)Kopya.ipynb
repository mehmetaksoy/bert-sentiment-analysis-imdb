{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Hücre 1: Gerekli Kütüphanelerin Spesifik Versiyonlarla Kurulumu\n",
        "\n",
        "* **Amaç:**\n",
        "    Projenin stabil bir şekilde çalışması için, daha önceki denemelerde uyumlu olduğu ve başarılı sonuçlar verdiği teyit edilen spesifik kütüphane versiyonlarının (özellikle NumPy, Datasets ve Transformers için) Google Colab ortamına kurulması. Bu yaklaşım, Colab'ın varsayılan veya `pip`'in otomatik olarak seçtiği versiyonlardan kaynaklanabilecek uyumsuzluk ve çalışma zamanı hatalarını en aza indirmeyi hedefler.\n",
        "\n",
        "* **Yapılan İşlemler:**\n",
        "    1.  Kuruluma başlamadan önce, Colab çalışma zamanının **\"Disconnect and delete runtime\"** seçeneği ile tamamen sıfırlanması şiddetle tavsiye edilir. Bu, olası kütüphane kalıntılarından ve çakışmalarından arınmış temiz bir ortam sağlar.\n",
        "    2.  `pip install -q` komutları kullanılarak aşağıdaki kütüphaneler belirtilen versiyonlara sabitlenir:\n",
        "        * **NumPy:** `1.26.4` versiyonuna. Bu versiyonun, `datasets` kütüphanesinin (özellikle 3.6.0 versiyonu) daha önceki `copy=False` hatasını vermeden çalışmasını sağladığı gözlemlenmiştir.\n",
        "        * **Datasets:** `3.6.0` versiyonuna. Bu versiyonun NumPy 1.26.4 ile uyumlu olduğu ve IMDB veri setini sorunsuz yüklediği teyit edilmiştir.\n",
        "        * **Transformers:** `4.48.3` versiyonuna. Bu versiyon, belirtilen Datasets ve NumPy versiyonlarıyla stabil bir şekilde çalışmıştır.\n",
        "    3.  Diğer yardımcı kütüphaneler olan `scikit-learn`, `matplotlib` ve `seaborn`, versiyon belirtilmeden kurulur. `pip`, bu kütüphanelerin yukarıda sabitlenen ana kütüphanelerle uyumlu en son versiyonlarını seçecektir.\n",
        "    4.  `pandas` kütüphanesi `pip` ile ayrıca kurulmaz; Colab'ın kendi sağladığı varsayılan versiyonun kullanılması hedeflenir.\n",
        "\n",
        "* **Uygulama Detayları/Sonuçlar:**\n",
        "    * Bu hücre çalıştırıldığında, belirtilen kütüphane versiyonları Colab ortamına kurulur.\n",
        "    * Kurulum sırasında `pip`'in bağımlılık çözümleyicisi, bazı uyarılar gösterebilir (örn: `thinc` kütüphanesinin farklı bir NumPy versiyonu beklemesi, `gcsfs`'in farklı bir `fsspec` beklemesi veya `torch` ile ilgili CUDA uyarıları). Ancak, bu spesifik versiyon kombinasyonuyla BERT modelinin baştan sona (veri yükleme, tokenizasyon, eğitim, değerlendirme) çalıştığı teyit edildiği için bu uyarılar genellikle göz ardı edilebilir.\n",
        "    * Hücrenin sonunda, yapılan kurulumların Colab ortamına tam olarak yansıması ve kütüphanelerin doğru şekilde yüklenmesi için kullanıcının **Colab çalışma zamanını \"Restart runtime\" ile KESİNLİKLE yeniden başlatması** gerektiği ve yeniden başlattıktan sonra bu Hücre 1'in **tekrar çalıştırılmaması**, doğrudan Hücre 2'ye (kütüphane importları) geçilmesi gerektiği önemle vurgulanır. Bu adımlar, stabil ve hatasız bir çalışma ortamı elde etmek için kritik öneme sahiptir."
      ],
      "metadata": {
        "id": "NRLBwAQhWPig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 1 (Başarılı Versiyonları Zorlayarak Kurulum)\n",
        "\n",
        "# Lütfen bu hücreyi çalıştırmadan önce \"Disconnect and delete runtime\" ile\n",
        "# Colab çalışma zamanını tamamen sıfırladığınızdan emin olun!\n",
        "\n",
        "print(\"Başarılı olduğu bilinen kütüphane versiyonları kuruluyor...\")\n",
        "\n",
        "# 1. NumPy'yi 1.26.4'e sabitleyelim.\n",
        "!pip install numpy==1.26.4 -q\n",
        "print(\"NumPy 1.26.4 kuruldu.\")\n",
        "\n",
        "# 2. Datasets'i 3.6.0'a sabitleyelim.\n",
        "!pip install datasets==3.6.0 -q\n",
        "print(\"Datasets 3.6.0 kuruldu.\")\n",
        "\n",
        "# 3. Transformers'ı 4.48.3'e sabitleyelim.\n",
        "!pip install transformers==4.48.3 -q\n",
        "print(\"Transformers 4.48.3 kuruldu.\")\n",
        "\n",
        "# 4. Diğer yardımcı kütüphaneler\n",
        "!pip install scikit-learn matplotlib seaborn -q\n",
        "print(\"Scikit-learn, Matplotlib, Seaborn kuruldu/güncellendi.\")\n",
        "\n",
        "print(\"\\nKütüphane kurulumları (Hücre 1) tamamlandı.\")\n",
        "print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "print(\"!!! LÜTFEN ŞİMDİ Colab Çalışma Zamanını (Runtime -> Restart runtime)      !!!\")\n",
        "print(\"!!! KESİNLİKLE YENİDEN BAŞLATIN.                                         !!!\")\n",
        "print(\"!!! Yeniden başlattıktan sonra Hücre 1'i TEKRAR ÇALIŞTIRMAYIN,           !!!\")\n",
        "print(\"!!! doğrudan Hücre 2'ye (Kütüphane Importları) geçin.                   !!!\")\n",
        "print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")"
      ],
      "metadata": {
        "id": "KDzLMaKLSms2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hücre 2: Kütüphanelerin Import Edilmesi, Versiyon Kontrolü ve Cihaz Belirleme\n",
        "\n",
        "* **Amaç:**\n",
        "    Çalışma zamanı yeniden başlatıldıktan sonra, Hücre 1'de kurulan ve Colab'da zaten mevcut olan kütüphaneleri Python ortamına dahil etmek. Kullanılan kütüphanelerin versiyonlarını teyit etmek ve model eğitimi/çıkarımı için uygun donanım cihazını (CPU/GPU) belirlemek.\n",
        "\n",
        "* **Yapılan İşlemler:**\n",
        "    1.  Gerekli tüm Python kütüphaneleri (`transformers`, `datasets`, `torch`, `numpy`, `pandas`, `sklearn`, `matplotlib`, `seaborn`, `fsspec`, `time`, `sys`) `import` edilir.\n",
        "    2.  Hugging Face `datasets` ve `transformers` kütüphanelerinden sıkça kullanılacak sınıflar (`load_dataset`, `AutoTokenizer`, `AutoModelForSequenceClassification`, `TrainingArguments`, `Trainer`) ayrıca import edilir.\n",
        "    3.  `torch.cuda.is_available()` ile GPU varlığı kontrol edilir ve `device` değişkeni `cuda` veya `cpu` olarak ayarlanır.\n",
        "    4.  Python uyarıları (`warnings`) isteğe bağlı olarak bastırılır.\n",
        "    5.  Ana kütüphanelerin (`Python`, `PyTorch`, `Transformers`, `Datasets`, `Numpy`, `Pandas`, `Scikit-learn`, `Matplotlib`, `Seaborn`, `fsspec`) versiyonları ekrana yazdırılır.\n",
        "    6.  Kullanılacak cihaz (`device`) ve eğer GPU aktifse, CUDA ve cuDNN versiyonları gibi ek donanım bilgileri gösterilir.\n",
        "\n",
        "* **Uygulama Detayları/Sonuçlar:**\n",
        "    * Bu hücrenin sorunsuz çalışması, Hücre 1'deki kurulumların ve ardından yapılan çalışma zamanı yeniden başlatmanın ortamı doğru şekilde hazırladığını gösterir.\n",
        "    * Çıktıda listelenen kütüphane versiyonları, projenin hangi ortamda başarıyla çalıştığını belgeler. En son başarılı çalıştırmanızdaki versiyonlar (Python 3.11.11, PyTorch 2.5.1+cu124, Transformers 4.48.3, Datasets 3.6.0, Numpy 1.26.4, Pandas 2.2.2, fsspec 2024.10.0) burada da görülmelidir. Bu versiyonların, özellikle `Datasets` 3.6.0 ve `Numpy` 1.26.4'ün, daha önceki uyumluluk sorunlarını aştığı görülmüştür.\n",
        "    * \"KULLANILACAK CİHAZ\" satırında `cuda` (ve aktif GPU adı, örn: NVIDIA L4) veya `cpu` bilgisi yer alır."
      ],
      "metadata": {
        "id": "fH-6XPOHM2ZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 2 (Düzeltilmiş): Kütüphanelerin Import Edilmesi, Versiyon Kontrolü ve Cihaz Belirleme\n",
        "\n",
        "# Temel ve Hugging Face kütüphanelerinin import edilmesi\n",
        "import transformers\n",
        "import datasets\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn # scikit-learn'ün ana modülü\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "import fsspec # fsspec versiyonunu da kontrol edelim\n",
        "import time   # Eğitim/çıkarım sürelerini ölçmek için\n",
        "import sys    # Python versiyonunu almak için eklendi\n",
        "\n",
        "# Hugging Face kütüphanelerinden sık kullanılacak modüller\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "# Cihazı belirleme: GPU varsa GPU, yoksa CPU kullanılacak.\n",
        "# Colab'da GPU kullanmak için: Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# İsteğe bağlı: Daha temiz bir çıktı için uyarıları bastırma\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "print(\"Kütüphaneler başarıyla import edildi.\")\n",
        "print(\"-\" * 50)\n",
        "print(\"KULLANILAN KÜTÜPHANE VERSİYONLARI:\")\n",
        "# Python versiyonu sys modülü kullanılarak düzeltildi:\n",
        "print(f\"  Python Versiyonu (sys.version): {sys.version.split()[0]}\")\n",
        "print(f\"  PyTorch: {torch.__version__}\")\n",
        "print(f\"  Transformers: {transformers.__version__}\")\n",
        "print(f\"  Datasets: {datasets.__version__}\")\n",
        "print(f\"  Numpy: {np.__version__}\")\n",
        "print(f\"  Pandas: {pd.__version__}\")\n",
        "print(f\"  Scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"  Matplotlib: {matplotlib.__version__}\")\n",
        "print(f\"  Seaborn: {sns.__version__}\")\n",
        "print(f\"  fsspec: {fsspec.__version__}\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"KULLANILACAK CİHAZ: {device}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# GPU varsa, CUDA ve cuDNN versiyonlarını da yazdıralım (bilgi amaçlı)\n",
        "if device.type == 'cuda':\n",
        "    print(f\"  CUDA Versiyonu (torch.version.cuda): {torch.version.cuda}\")\n",
        "    print(f\"  cuDNN Versiyonu (torch.backends.cudnn.version()): {torch.backends.cudnn.version()}\")\n",
        "    print(f\"  Kullanılabilir GPU Sayısı: {torch.cuda.device_count()}\")\n",
        "    print(f\"  Aktif GPU Adı: {torch.cuda.get_device_name(0)}\")\n",
        "    print(\"-\" * 50)\n",
        "else:\n",
        "    print(\"Uyarı: GPU bulunamadı veya aktif değil. Model eğitimi CPU üzerinde daha yavaş olacaktır.\")\n",
        "    print(\"Colab'da GPU'yu aktifleştirmek için 'Runtime' -> 'Change runtime type' menüsünü kullanabilirsiniz.\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "goNWz2PrM44w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hücre 3: IMDB Veri Setinin Yüklenmesi ve Temel İnceleme\n",
        "\n",
        "* **Amaç:**\n",
        "    IMDB film yorumları veri setini Hugging Face `datasets` kütüphanesiyle yüklemek. Veri setinin yapısını, özelliklerini (\"text\", \"label\") ve etiketleme şemasını (0: \"neg\", 1: \"pos\") incelemek. Sınıflandırma görevi için gereksiz olan \"unsupervised\" (etiketsiz) bölümünü çıkarmak.\n",
        "\n",
        "* **Yapılan İşlemler:**\n",
        "    1.  `load_dataset(\"imdb\")` fonksiyonu ile IMDB veri seti (`imdb_dataset_raw`) yüklenir.\n",
        "    2.  Yüklenen ham `DatasetDict` yapısı (`train`, `test`, `unsupervised` alt kümeleri) yazdırılır.\n",
        "    3.  Eğitim setindeki `label` özelliğinden `ClassLabel` objesi (`label_feature`) alınır ve etiketlerin metin karşılıkları teyit edilir. Bu değişken, sonraki hücrelerde etiket isimlerine erişim için global olarak da kullanılabilir.\n",
        "    4.  `unsupervised` alt kümesi, `imdb_dataset_raw`'dan çıkarılarak sadece `train` ve `test` alt kümelerini içeren `imdb_dataset_processed` adlı yeni bir `DatasetDict` oluşturulur.\n",
        "    5.  İşlenmiş veri setinin yapısı ve eğitim setinden örnek bir yorum ile etiketi gösterilir.\n",
        "\n",
        "* **Uygulama Detayları/Sonuçlar:**\n",
        "    * Veri setinin sorunsuz bir şekilde indirilip yüklenmesi beklenir (`Datasets 3.6.0` ve `NumPy 1.26.4` kombinasyonuyla `Invalid pattern` hatası alınmamalıdır).\n",
        "    * `imdb_dataset_processed` objesi, sonraki adımda eğitim, doğrulama ve test setlerine bölünmek üzere hazır hale gelir. `label_feature` objesi de etiketleme bilgisini taşır."
      ],
      "metadata": {
        "id": "2nwstyclM8R0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 3: IMDB Veri Setinin Yüklenmesi ve Temel İnceleme\n",
        "\n",
        "print(\"IMDB veri seti yükleniyor...\")\n",
        "try:\n",
        "    imdb_dataset_raw = load_dataset(\"imdb\")\n",
        "    print(\"IMDB veri seti başarıyla yüklendi.\")\n",
        "    print(\"\\nYüklenen Ham Veri Seti Yapısı:\")\n",
        "    print(imdb_dataset_raw)\n",
        "\n",
        "    # label_feature'ı global yaparak diğer hücrelerden erişilebilir kılalım.\n",
        "    # Bu, Hücre 2'de import edilen datasets kütüphanesini kullanır.\n",
        "    global label_feature\n",
        "    label_feature = imdb_dataset_raw['train'].features['label']\n",
        "\n",
        "    print(\"\\nEtiket Bilgisi (ClassLabel):\")\n",
        "    print(label_feature)\n",
        "    print(f\"Etiket 0: {label_feature.int2str(0)}\") # neg\n",
        "    print(f\"Etiket 1: {label_feature.int2str(1)}\") # pos\n",
        "\n",
        "    # 'unsupervised' alt kümesini çıkararak yeni bir DatasetDict oluşturalım\n",
        "    keys_to_keep = ['train', 'test']\n",
        "    imdb_dataset_processed = datasets.DatasetDict(\n",
        "        {k: imdb_dataset_raw[k] for k in keys_to_keep if k in imdb_dataset_raw}\n",
        "    )\n",
        "\n",
        "    if 'unsupervised' not in imdb_dataset_processed and 'unsupervised' in imdb_dataset_raw:\n",
        "        print(\"\\n'unsupervised' alt kümesi başarıyla işlenmiş veri setinden çıkarıldı.\")\n",
        "    elif 'unsupervised' not in imdb_dataset_raw:\n",
        "        print(\"\\n'unsupervised' alt kümesi zaten ham veri setinde bulunmuyordu.\")\n",
        "    else: # Bu durumun oluşmaması gerekir eğer pop veya yeniden oluşturma doğruysa\n",
        "        print(\"\\n'unsupervised' alt kümesi işlenmiş veri setinde hala mevcut veya bir sorun oluştu.\")\n",
        "\n",
        "    print(\"\\nİşlenmiş (Relevant) Veri Seti Yapısı:\")\n",
        "    print(imdb_dataset_processed)\n",
        "\n",
        "    if 'train' in imdb_dataset_processed and len(imdb_dataset_processed['train']) > 0:\n",
        "        print(\"\\nEğitim setinden ilk örnek:\")\n",
        "        example = imdb_dataset_processed['train'][0]\n",
        "        print(f\"  Metin: {example['text'][:200]}...\") # İlk 200 karakter\n",
        "        print(f\"  Etiket: {example['label']} ({label_feature.int2str(example['label'])})\")\n",
        "    else:\n",
        "        print(\"\\nEğitim seti bulunamadı veya boş.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nVeri seti yüklenirken veya işlenirken bir hata oluştu: {e}\")\n",
        "    # Hata durumunda değişkenlerin None olarak ayarlanması, sonraki hücrelerde kontrolü kolaylaştırır.\n",
        "    imdb_dataset_processed = None\n",
        "    if 'label_feature' in globals(): del label_feature # Eğer tanımlandıysa silelim"
      ],
      "metadata": {
        "id": "2DLA3KmwM7Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hücre 4: Veri Setini Eğitim, Doğrulama ve Test Alt Kümelerine Ayırma (scikit-learn ile)\n",
        "\n",
        "* **Amaç:**\n",
        "    Modelin eğitimi sırasında performansını izlemek ve aşırı öğrenmeyi önlemek/tespit etmek için `imdb_dataset_processed` içindeki `train` alt kümesinden bir doğrulama (validation) seti oluşturmak. Bu işlem, `scikit-learn` kütüphanesi kullanılarak yapılır.\n",
        "\n",
        "* **Yapılan İşlemler:**\n",
        "    1.  `sklearn.model_selection.train_test_split` fonksiyonu (`sk_train_test_split` olarak) import edilir.\n",
        "    2.  `imdb_dataset_processed['train']` setindeki metinler ve etiketler Python listelerine alınır.\n",
        "    3.  `sk_train_test_split` ile verinin %20'si doğrulama, %80'i yeni eğitim seti olarak, sınıfların orantılı dağılımı (`stratify`) korunarak ve tekrarlanabilirlik (`random_state=42`) sağlanarak bölünür.\n",
        "    4.  Bölünmüş listeler, `datasets.Dataset.from_dict()` ile tekrar Hugging Face `Dataset` objelerine dönüştürülürken, orijinal `features` bilgisi (özellikle `label` için `ClassLabel` tanımı) yeni `Dataset`'lere aktarılır.\n",
        "    5.  Yeni `train`, `validation` ve orijinal `test` setleri, `final_imdb_dataset` adlı bir `DatasetDict` içinde birleştirilir.\n",
        "    6.  Her alt kümedeki örnek sayıları ve sınıf dağılımları (`label_feature.int2str` kullanılarak) yazdırılarak kontrol edilir.\n",
        "\n",
        "* **Uygulama Detayları/Sonuçlar:**\n",
        "    * `final_imdb_dataset` objesi, `train` (20000 örnek), `validation` (5000 örnek) ve `test` (25000 örnek) alt kümelerini içerir.\n",
        "    * Her alt küme için etiket dağılımlarının dengeli olduğu (örn: eğitimde 10000 \"neg\"/10000 \"pos\") teyit edilir. Bu, modelin sonraki adımlarda adil bir şekilde eğitilmesi için önemlidir."
      ],
      "metadata": {
        "id": "KZFcUTRRNSft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 4 (Düzeltilmiş - scikit-learn ile Bölme): Veri Setini Eğitim, Doğrulama ve Test Alt Kümelerine Ayırma\n",
        "\n",
        "from sklearn.model_selection import train_test_split as sk_train_test_split\n",
        "\n",
        "# Gerekli değişkenlerin bir önceki hücreden geldiğini varsayıyoruz\n",
        "if 'imdb_dataset_processed' in globals() and imdb_dataset_processed is not None and 'train' in imdb_dataset_processed and \\\n",
        "   'label_feature' in globals() and label_feature is not None:\n",
        "    print(\"Mevcut 'train' seti, scikit-learn kullanılarak 'validation' seti oluşturmak üzere bölünüyor...\")\n",
        "\n",
        "    train_texts_list = imdb_dataset_processed['train']['text']\n",
        "    train_labels_list = imdb_dataset_processed['train']['label']\n",
        "\n",
        "    new_train_texts, validation_texts, new_train_labels, validation_labels = sk_train_test_split(\n",
        "        train_texts_list,\n",
        "        train_labels_list,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=train_labels_list\n",
        "    )\n",
        "\n",
        "    original_features = imdb_dataset_processed['train'].features\n",
        "\n",
        "    train_dataset_new = datasets.Dataset.from_dict(\n",
        "        {'text': new_train_texts, 'label': new_train_labels},\n",
        "        features=original_features\n",
        "    )\n",
        "    validation_dataset_new = datasets.Dataset.from_dict(\n",
        "        {'text': validation_texts, 'label': validation_labels},\n",
        "        features=original_features\n",
        "    )\n",
        "\n",
        "    # final_imdb_dataset'i global yapalım ki diğer hücrelerden erişilebilsin\n",
        "    global final_imdb_dataset\n",
        "    final_imdb_dataset = datasets.DatasetDict({\n",
        "        'train': train_dataset_new,\n",
        "        'validation': validation_dataset_new,\n",
        "        'test': imdb_dataset_processed['test']\n",
        "    })\n",
        "\n",
        "    print(\"\\nNihai Veri Seti Yapısı (Eğitim, Doğrulama, Test):\")\n",
        "    print(final_imdb_dataset)\n",
        "\n",
        "    print(f\"\\nEğitim seti örnek sayısı: {len(final_imdb_dataset['train'])}\")\n",
        "    print(f\"Doğrulama seti örnek sayısı: {len(final_imdb_dataset['validation'])}\")\n",
        "    print(f\"Test seti örnek sayısı: {len(final_imdb_dataset['test'])}\")\n",
        "\n",
        "    def get_label_distribution(dataset_split, current_label_feature):\n",
        "        if not dataset_split: return \"Veri seti objesi None.\"\n",
        "        if len(dataset_split) == 0: return \"Veri seti boş.\"\n",
        "        try:\n",
        "            labels = dataset_split['label']\n",
        "            series = pd.Series(labels).value_counts().sort_index()\n",
        "            # current_label_feature'ın None olup olmadığını ve int2str metoduna sahip olup olmadığını kontrol et\n",
        "            if current_label_feature is not None and hasattr(current_label_feature, 'int2str'):\n",
        "                 return series.rename(index=current_label_feature.int2str)\n",
        "            return series\n",
        "        except Exception as e: return f\"Etiket dağılımı hesaplanırken hata: {e}\"\n",
        "\n",
        "    print(\"\\nEğitim Seti Etiket Dağılımı:\")\n",
        "    print(get_label_distribution(final_imdb_dataset['train'], label_feature))\n",
        "\n",
        "    print(\"\\nDoğrulama Seti Etiket Dağılımı:\")\n",
        "    print(get_label_distribution(final_imdb_dataset['validation'], label_feature))\n",
        "\n",
        "    print(\"\\nTest Seti Etiket Dağılımı:\")\n",
        "    print(get_label_distribution(final_imdb_dataset['test'], label_feature))\n",
        "else:\n",
        "    print(\"Hata: 'imdb_dataset_processed' veya 'label_feature' doğru şekilde tanımlanmamış. Bu hücre çalıştırılamadı.\")\n",
        "    if 'final_imdb_dataset' in globals(): del final_imdb_dataset"
      ],
      "metadata": {
        "id": "aAlSKtaQNR1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hücre 5: BERT Tokenizer'ının Yüklenmesi ve Veri Setinin Tokenizasyonu\n",
        "\n",
        "* **Amaç:**\n",
        "    Film yorumlarını (`final_imdb_dataset` içindeki metinleri) `bert-base-uncased` modelinin işleyebileceği sayısal girdilere (token ID'leri, attention mask vb.) dönüştürmek.\n",
        "\n",
        "* **Yapılan İşlemler:**\n",
        "    1.  Kullanılacak model adı (`model_checkpoint = \"bert-base-uncased\"`) belirlenir.\n",
        "    2.  `AutoTokenizer.from_pretrained()` ile bu modele ait tokenizer yüklenir.\n",
        "    3.  Tokenizer'ın çalışması örnek bir metinle test edilir.\n",
        "    4.  Tüm veri setini işlemek için bir `tokenize_function` tanımlanır. Bu fonksiyon, metinleri alır ve tokenizer ile `padding=\"max_length\"`, `truncation=True`, `max_length=256` (her yorum için maksimum 256 token) parametreleriyle tokenize eder.\n",
        "    5.  `final_imdb_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])` ile tokenizasyon tüm alt kümelere (`train`, `validation`, `test`) uygulanır ve orijinal `text` sütunu kaldırılır.\n",
        "    6.  `tokenized_datasets.set_format(\"torch\")` ile veri setinin formatı PyTorch tensörlerine ayarlanır.\n",
        "    7.  Tokenize edilmiş veri setinin yapısı ve eğitim setinden bir örnek (içerdiği tensörler ve `input_ids` uzunluğu) yazdırılır.\n",
        "\n",
        "* **Uygulama Detayları/Sonuçlar:**\n",
        "    * `bert-base-uncased` tokenizer'ı Hugging Face Hub'dan indirilir.\n",
        "    * `.map()` işlemi ilerleme çubuklarıyla birlikte tüm veri setini işler.\n",
        "    * `tokenized_datasets` içindeki her örnek, `label`'a ek olarak `input_ids`, `token_type_ids`, ve `attention_mask` sütunlarını içerir.\n",
        "    * Yazdırılan örnekte, `input_ids` uzunluğunun 256 olduğu ve verinin PyTorch tensörleri olarak formatlandığı teyit edilir. Bu hücrenin, mevcut stabil kütüphane ortamında (NumPy 1.26.4, Datasets 3.6.0) sorunsuz çalışması ve örnek yazdırma sırasında hata vermemesi beklenir."
      ],
      "metadata": {
        "id": "9nG3pTqoNaZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 5: BERT Tokenizer'ının Yüklenmesi ve Veri Setinin Tokenizasyonu\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "print(f\"Kullanılacak model checkpoint: {model_checkpoint}\")\n",
        "\n",
        "# final_imdb_dataset'in Hücre 4'te tanımlandığını varsayıyoruz\n",
        "if 'final_imdb_dataset' in globals() and final_imdb_dataset is not None:\n",
        "    try:\n",
        "        # tokenizer'ı global yapalım ki diğer hücrelerden erişilebilsin\n",
        "        global tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "        print(f\"\\n'{model_checkpoint}' için tokenizer başarıyla yüklendi.\")\n",
        "\n",
        "        sample_text = \"Hello, this is a sample sentence for tokenization!\"\n",
        "        encoded_sample = tokenizer(sample_text)\n",
        "        print(f\"\\nÖrnek metin: '{sample_text}'\")\n",
        "        print(f\"Tokenize edilmiş hali (input_ids): {encoded_sample['input_ids']}\")\n",
        "        print(f\"Token ID'lerinin tekrar metne dönüştürülmüş hali: {tokenizer.convert_ids_to_tokens(encoded_sample['input_ids'])}\")\n",
        "\n",
        "        def tokenize_function(examples):\n",
        "            return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
        "\n",
        "        print(f\"\\nVeri seti tokenize ediliyor (max_length=256)... Bu işlem biraz zaman alabilir.\")\n",
        "\n",
        "        # tokenized_datasets'i global yapalım\n",
        "        global tokenized_datasets\n",
        "        tokenized_datasets = final_imdb_dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=[\"text\"]\n",
        "        )\n",
        "\n",
        "        tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "        print(\"\\nVeri seti başarıyla tokenize edildi ve formatı PyTorch tensörlerine ayarlandı.\")\n",
        "        print(\"Tokenize edilmiş veri seti yapısı:\")\n",
        "        print(tokenized_datasets)\n",
        "\n",
        "        if 'train' in tokenized_datasets and len(tokenized_datasets['train']) > 0:\n",
        "            print(\"\\nTokenize edilmiş eğitim setinden ilk örnek:\")\n",
        "            example_item = tokenized_datasets['train'][0]\n",
        "            print(example_item)\n",
        "            if 'input_ids' in example_item and hasattr(example_item['input_ids'], '__len__'):\n",
        "                 print(f\"Input IDs uzunluğu: {len(example_item['input_ids'])}\")\n",
        "            else:\n",
        "                print(\"Örnekte 'input_ids' bulunamadı veya uzunluğu alınamadı.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTokenizer yüklenirken veya tokenizasyon sırasında bir genel hata oluştu: {e}\")\n",
        "        if 'tokenized_datasets' in globals(): del tokenized_datasets\n",
        "        if 'tokenizer' in globals(): del tokenizer\n",
        "else:\n",
        "    print(\"Hata: 'final_imdb_dataset' bulunamadığı için tokenizasyon işlemi başlatılamadı.\")\n",
        "    if 'tokenized_datasets' in globals(): del tokenized_datasets\n",
        "    if 'tokenizer' in globals(): del tokenizer"
      ],
      "metadata": {
        "id": "c6sIO_Y0NXS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hücre 6: Önceden Eğitilmiş BERT Modelinin Yüklenmesi\n",
        "\n",
        "* **Amaç:**\n",
        "    Duygu analizi görevi için `bert-base-uncased` önceden eğitilmiş modelini yüklemek ve ikili sınıflandırma (pozitif/negatif) problemine uygun bir sınıflandırma başlığıyla yapılandırmak.\n",
        "\n",
        "* **Yapılan İşlemler:**\n",
        "    1.  `AutoModelForSequenceClassification.from_pretrained()` ile `model_checkpoint` (\"bert-base-uncased\") ve `num_labels=2` parametreleriyle model yüklenir. `num_labels=2`, modelin üzerine eklenecek sınıflandırma katmanının 2 çıktıya sahip olacağını belirtir; bu katmanın ağırlıkları rastgele başlatılır.\n",
        "    2.  Yüklenen `model`, `model.to(device)` ile GPU'ya (Hücre 2'de belirlenen `cuda` cihazına) taşınır.\n",
        "    3.  Modelin başarıyla yüklendiği, cihaza taşındığı, toplam/eğitilebilir parametre sayısı ve sınıflandırıcısının doğru sayıda etiket (2) için yapılandırıldığı teyit edilir.\n",
        "\n",
        "* **Uygulama Detayları/Sonuçlar:**\n",
        "    * Model ağırlıkları Hugging Face Hub'dan indirilir.\n",
        "    * Sınıflandırma katmanının ağırlıklarının rastgele başlatıldığına ve modelin bu görev için eğitilmesi gerektiğine dair bir uyarı (`Some weights ... were not initialized...`) görülür; bu beklenen bir durumdur.\n",
        "    * Modelin GPU'ya taşındığı, ~109.5 milyon parametresinin olduğu ve sınıflandırıcısının 2 etiket için doğru yapılandırıldığı doğrulanır. `model` değişkeni, eğitilmek üzere hazır hale gelir."
      ],
      "metadata": {
        "id": "rVj4wdvMNiMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 6: Önceden Eğitilmiş BERT Modelinin Yüklenmesi\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# model_checkpoint, num_labels, device değişkenlerinin önceki hücrelerde tanımlandığını varsayıyoruz\n",
        "# num_labels'ı burada tekrar tanımlayalım veya global olduğundan emin olalım.\n",
        "# Hücre 5'te model_checkpoint, Hücre 2'de device tanımlandı.\n",
        "# num_labels'ı burada tanımlamak daha güvenli olabilir.\n",
        "num_labels = 2\n",
        "\n",
        "if 'model_checkpoint' in globals() and 'num_labels' in globals() and 'device' in globals():\n",
        "    print(f\"'{model_checkpoint}' modeli '{num_labels}' etiket ile sınıflandırma görevi için yükleniyor...\")\n",
        "    try:\n",
        "        # model'i global yapalım\n",
        "        global model\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_checkpoint,\n",
        "            num_labels=num_labels\n",
        "        )\n",
        "\n",
        "        model.to(device)\n",
        "\n",
        "        print(f\"\\nModel başarıyla yüklendi ve '{device}' cihazına taşındı.\")\n",
        "\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        print(f\"\\nModeldeki toplam parametre sayısı: {total_params:,}\")\n",
        "        print(f\"Modeldeki eğitilebilir parametre sayısı: {trainable_params:,}\")\n",
        "\n",
        "        if hasattr(model.config, 'num_labels'):\n",
        "            print(f\"Modelin yapılandırmasındaki etiket sayısı: {model.config.num_labels}\")\n",
        "        if hasattr(model, 'classifier') and hasattr(model.classifier, 'out_features'):\n",
        "             print(f\"Sınıflandırıcı katmanının çıktı boyutu: {model.classifier.out_features}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nModel yüklenirken bir hata oluştu: {e}\")\n",
        "        if 'model' in globals(): del model\n",
        "else:\n",
        "    print(\"Hata: Gerekli değişkenler ('model_checkpoint', 'num_labels', 'device') bulunamadı.\")\n",
        "    if 'model' in globals(): del model"
      ],
      "metadata": {
        "id": "-i5CDlnNNjGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hücre 7: Eğitim Argümanlarının Tanımlanması, Metrik Hesaplama Fonksiyonu ve Modelin Eğitilmesi (Epoch Bazında Değerlendirme ile)\n",
        "\n",
        "* **Amaç:**\n",
        "    BERT modelini, hazırladığımız tokenize edilmiş IMDB veri seti üzerinde duygu analizi görevi için eğitmek (fine-tuning). Bu süreç için eğitim hiperparametrelerini, her epoch sonunda değerlendirme metriklerini hesaplayacak bir fonksiyonu ve eğitimi yönetecek `Trainer` API'sini yapılandırmak. Bu güncellenmiş versiyonda, her epoch sonunda modelin doğrulama seti üzerindeki performansının değerlendirilmesi ve en iyi modelin eğitim sonunda otomatik olarak yüklenmesi hedeflenmektedir.\n",
        "\n",
        "* **Yapılan İşlemler:**\n",
        "    1.  **`compute_metrics` Fonksiyonu Tanımlanması:**\n",
        "        * Modelin tahminlerini ve gerçek etiketleri alarak Accuracy, Precision, Recall, F1-Score, Specificity ve AUC metriklerini hesaplar. Olası `zero_division` ve tek-sınıf durumları için düzeltmeler içerir.\n",
        "    2.  **`TrainingArguments` Tanımlanması (Epoch Bazında Değerlendirme ile):**\n",
        "        * `output_dir`: Çıktıların kaydedileceği dizin (örn: `\"./results_with_epoch_eval\"`).\n",
        "        * `num_train_epochs`: Toplam eğitim epoch sayısı (örn: 3).\n",
        "        * `per_device_train_batch_size` / `per_device_eval_batch_size`: Batch boyutları.\n",
        "        * `logging_strategy=\"epoch\"`: Metriklerin her epoch sonunda loglanmasını sağlar.\n",
        "        * **`evaluation_strategy=\"epoch\"`:** Modelin her epoch sonunda doğrulama seti üzerinde değerlendirilmesini sağlar.\n",
        "        * **`save_strategy=\"epoch\"`:** Model checkpoint'inin her epoch sonunda kaydedilmesini sağlar.\n",
        "        * **`load_best_model_at_end=True`:** Eğitim tamamlandığında, `metric_for_best_model`'e göre en iyi performansı gösteren model checkpoint'inin otomatik olarak yüklenmesini sağlar.\n",
        "        * **`metric_for_best_model=\"accuracy\"`:** En iyi modeli belirlemek için \"accuracy\" metriğini kullanır.\n",
        "        * `save_total_limit`: Kaydedilecek maksimum checkpoint sayısı (örn: 2).\n",
        "    3.  **`Trainer` Objesinin Oluşturulması:** `model`, güncellenmiş `args`, veri setleri, `tokenizer` ve `compute_metrics` fonksiyonu ile `Trainer` objesi oluşturulur.\n",
        "    4.  **Modelin Eğitilmesi:** `trainer.train()` ile model eğitimi başlatılır, süre ölçülür. Eğitim sonunda genel metrikler yazdırılır ve en iyi model (eğer `load_best_model_at_end=True` ise `trainer.model` içinde yüklü olan) ve tokenizer ayrıca kaydedilir.\n",
        "\n",
        "* **Uygulama Detayları/Sonuçlar (Beklenen):**\n",
        "    * `TrainingArguments`'ın bu güncellenmiş haliyle, `transformers 4.48.3` versiyonunun `evaluation_strategy` ve `save_strategy` argümanlarını `\"epoch\"` değeriyle kabul etmesi ve `TypeError` vermemesi beklenir.\n",
        "    * Eğitim sırasında her epoch sonunda hem eğitim kaybı hem de **doğrulama kaybı ve diğer doğrulama metriklerinin (accuracy, F1 vb.)** loglandığı görülecektir.\n",
        "    * Eğitim tamamlandığında, `trainer` objesi en iyi performansı veren modeli otomatik olarak yüklemiş olacaktır.\n",
        "    * `log_history` içeriği, her epoch için hem eğitim hem de doğrulama loglarını içereceğinden, Hücre 10'daki grafikler çok daha bilgilendirici olacaktır."
      ],
      "metadata": {
        "id": "XpDkuHE4Nnab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 7 (Epoch Bazında Değerlendirme ile Güncellenmiş TrainingArguments - Düzeltilmiş `if` koşulu ile):\n",
        "# Eğitim Argümanlarının Tanımlanması, Metrik Hesaplama Fonksiyonu ve Modelin Eğitilmesi\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
        "# numpy, torch, time zaten Hücre 2'de import edilmişti.\n",
        "# num_labels, model, tokenized_datasets, tokenizer, device değişkenlerinin\n",
        "# önceki hücrelerde doğru şekilde tanımlandığını ve erişilebilir olduğunu varsayıyoruz.\n",
        "\n",
        "# 1. Performans Metriklerini Hesaplama Fonksiyonu\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    # num_labels'ın (Hücre 6'da tanımlanmıştı) bu scope'ta erişilebilir olması gerekir.\n",
        "    # Ya global yapın ya da model.config.num_labels kullanın.\n",
        "    # Şimdilik modelin config'inden alalım, daha güvenli.\n",
        "    current_num_labels = model.config.num_labels if 'model' in globals() and model else 2 # Fallback\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    try:\n",
        "        cm = confusion_matrix(labels, preds, labels=[0, 1])\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "    except ValueError:\n",
        "        tn, fp, fn, tp = 0,0,0,0\n",
        "        if len(np.unique(labels)) == 1:\n",
        "            if np.unique(labels)[0] == 0 and (len(np.unique(preds))==0 or (len(np.unique(preds))==1 and np.unique(preds)[0] == 0)): tn = len(labels)\n",
        "            if np.unique(labels)[0] == 1 and (len(np.unique(preds))==0 or (len(np.unique(preds))==1 and np.unique(preds)[0] == 1)): tp = len(labels)\n",
        "\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "\n",
        "    auc_value = 0.0\n",
        "    if pred.predictions.ndim == 2 and pred.predictions.shape[1] == current_num_labels:\n",
        "        try:\n",
        "            logits_tensor = torch.tensor(pred.predictions)\n",
        "            probs = torch.softmax(logits_tensor, dim=-1).cpu().numpy()\n",
        "            positive_class_probs = probs[:, 1]\n",
        "            auc_value = roc_auc_score(labels, positive_class_probs)\n",
        "        except ValueError:\n",
        "            auc_value = 0.0\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1,\n",
        "        'specificity': specificity, 'auc': auc_value\n",
        "    }\n",
        "\n",
        "# 2. Eğitim Argümanları (TrainingArguments) - Epoch bazında değerlendirme ile\n",
        "# Önceki çalışmanızdaki output_dir'den farklı bir isim kullanalım ki sonuçlar karışmasın.\n",
        "training_args_epoch_eval = TrainingArguments(\n",
        "    output_dir=\"./results_epoch_evaluation\", # Yeni çıktı dizini\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",             # <<-- HER EPOCH SONUNDA DEĞERLENDİRME\n",
        "    save_strategy=\"epoch\",                   # <<-- HER EPOCH SONUNDA CHECKPOINT KAYDETME\n",
        "    load_best_model_at_end=True,             # <<-- Eğitim sonunda en iyi modeli yükle\n",
        "    metric_for_best_model=\"accuracy\",        # En iyi modeli 'accuracy' metriğine göre belirle\n",
        "    save_total_limit=2, # En iyi ve son checkpoint'i tutabilir\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "print(f\"\\nEğitim argümanları (epoch bazında değerlendirme ile) tanımlandı. Çıktı dizini: {training_args_epoch_eval.output_dir}\")\n",
        "\n",
        "# 3. Trainer Objesinin Oluşturulması - DÜZELTİLMİŞ `if` KOŞULU\n",
        "if 'model' in globals() and model is not None and \\\n",
        "   'tokenized_datasets' in globals() and tokenized_datasets is not None and \\\n",
        "   'tokenizer' in globals() and tokenizer is not None: # 'device' ve 'training_time' kontrolü çıkarıldı\n",
        "\n",
        "    # trainer'ı global yapalım ki sonraki hücrelerde (örn: Hücre 8) kullanılabilsin\n",
        "    global trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args_epoch_eval,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"validation\"],\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "    print(\"\\nTrainer objesi başarıyla oluşturuldu.\")\n",
        "\n",
        "    print(\"\\nModel eğitimi (epoch bazında değerlendirme ile) başlatılıyor...\")\n",
        "    # device ve diğer bilgilerin Hücre 2'de zaten yazdırıldığını varsayıyoruz.\n",
        "    print(f\"Epoch sayısı: {training_args_epoch_eval.num_train_epochs}, Train Batch Size: {training_args_epoch_eval.per_device_train_batch_size}\")\n",
        "\n",
        "    # training_time'ı global yapalım\n",
        "    global training_time\n",
        "    start_time_train_epoch_eval = time.time()\n",
        "    try:\n",
        "        train_result = trainer.train()\n",
        "        end_time_train_epoch_eval = time.time()\n",
        "        training_time = end_time_train_epoch_eval - start_time_train_epoch_eval\n",
        "\n",
        "        print(f\"\\nEğitim tamamlandı! Toplam eğitim süresi: {training_time:.2f} saniye ({training_time/60:.2f} dakika).\")\n",
        "\n",
        "        if hasattr(train_result, 'metrics') and train_result.metrics:\n",
        "            print(\"Genel eğitim sonuç metrikleri (trainer.train() dönüşünden):\")\n",
        "            for key, value in train_result.metrics.items():\n",
        "                print(f\"  {key}: {value}\")\n",
        "\n",
        "        # load_best_model_at_end=True olduğu için, trainer.model artık en iyi modeli içeriyor.\n",
        "        # Bu en iyi modeli kaydedelim.\n",
        "        best_model_path = f\"{training_args_epoch_eval.output_dir}/best_model\"\n",
        "        trainer.save_model(best_model_path)\n",
        "        tokenizer.save_pretrained(best_model_path)\n",
        "        print(f\"Eğitim sonrası (en iyi) model ve tokenizer '{best_model_path}' adresine kaydedildi.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nModel eğitimi sırasında bir hata oluştu: {e}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        if 'train_result' not in globals(): train_result = None\n",
        "        training_time = None # Hata durumunda training_time'ı None yapalım\n",
        "else:\n",
        "    print(\"\\nModel, tokenize edilmiş veri seti veya tokenizer bulunamadığı için Trainer oluşturulamadı ve eğitim başlatılamadı.\")\n",
        "    if 'trainer' in globals(): del trainer\n",
        "    if 'train_result' not in globals(): train_result = None\n",
        "    training_time = None"
      ],
      "metadata": {
        "id": "UqeWrb4RNoh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hücre 8: Eğitilmiş Modelin Test Seti Üzerinde Değerlendirilmesi\n",
        "\n",
        "* **Amaç:**\n",
        "    Eğitimi tamamlanan BERT modelinin, daha önce hiç karşılaşmadığı test veri seti üzerindeki genelleme performansını, proje isterlerinde belirtilen tüm metrikler (Accuracy, Precision, Recall, F1, Specificity, AUC) açısından ölçmek.\n",
        "\n",
        "* **Yapılan İşlemler:**\n",
        "    1.  Bir önceki hücrede eğitimi tamamlanan `trainer` objesi kullanılır.\n",
        "    2.  `trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])` metodu çağrılarak, modelin test seti üzerindeki tahminleri alınır ve `compute_metrics` fonksiyonu (Hücre 7'de tanımlanmıştı) aracılığıyla performans metrikleri hesaplanır.\n",
        "    3.  Hesaplanan metrikler (`eval_loss`, `eval_accuracy`, `eval_precision` vb. içeren bir sözlük) ekrana okunaklı bir formatta yazdırılır.\n",
        "    4.  Bu metrikler, `bert_test_metrics` adlı bir değişkende saklanarak sonraki adımlarda (özellikle raporlama ve Hücre 9'daki AUC karşılaştırması için) kullanılabilir hale getirilir.\n",
        "\n",
        "* **Uygulama Detayları/Sonuçlar:**\n",
        "    * Test seti üzerinde değerlendirme yapılırken ilerleme çubuğu görülebilir.\n",
        "    * Çıktıda, test seti için hesaplanan Kayıp (Loss) ve diğer tüm performans metrikleri listelenir.\n",
        "    * En son başarılı çalıştırmada elde edilen sonuçlar (Test Accuracy: 0.9212, Test Precision: 0.9191, Test Recall: 0.9238, Test F1-Score: 0.9214, Test Specificity: 0.9186, Test AUC: 0.9751) modelin test verisi üzerinde iyi bir genelleme yaptığını göstermiştir."
      ],
      "metadata": {
        "id": "ZrxLEMrnNxuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 8: Eğitilmiş Modelin Test Seti Üzerinde Değerlendirilmesi\n",
        "\n",
        "print(\"Eğitilmiş model test seti üzerinde değerlendiriliyor...\")\n",
        "\n",
        "# Gerekli değişkenlerin varlığını kontrol edelim\n",
        "if 'trainer' in globals() and trainer is not None and \\\n",
        "   'tokenized_datasets' in globals() and \"test\" in tokenized_datasets:\n",
        "    try:\n",
        "        # eval_dataset parametresi ile test setini belirtiyoruz\n",
        "        test_metrics_output = trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])\n",
        "\n",
        "        print(\"\\nTest Seti Performans Metrikleri:\")\n",
        "        # trainer.evaluate() metriklerin başına 'eval_' ekler\n",
        "        print(f\"  Test Kaybı (Loss): {test_metrics_output.get('eval_loss', 'N/A'):.4f}\")\n",
        "        print(f\"  Test Accuracy: {test_metrics_output.get('eval_accuracy', 'N/A'):.4f}\")\n",
        "        print(f\"  Test Precision: {test_metrics_output.get('eval_precision', 'N/A'):.4f}\")\n",
        "        print(f\"  Test Recall: {test_metrics_output.get('eval_recall', 'N/A'):.4f}\")\n",
        "        print(f\"  Test F1-Score: {test_metrics_output.get('eval_f1', 'N/A'):.4f}\")\n",
        "        print(f\"  Test Specificity: {test_metrics_output.get('eval_specificity', 'N/A'):.4f}\")\n",
        "        print(f\"  Test AUC: {test_metrics_output.get('eval_auc', 'N/A'):.4f}\")\n",
        "\n",
        "        # Metrikleri globalde saklayalım ki Hücre 9'da kullanılabilsin\n",
        "        global bert_test_metrics\n",
        "        bert_test_metrics = test_metrics_output\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTest seti değerlendirmesi sırasında bir hata oluştu: {e}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        bert_test_metrics = None # Hata durumunda None olarak ayarla\n",
        "else:\n",
        "    print(\"Hata: 'trainer' objesi veya 'tokenized_datasets['test']' bulunamadı.\")\n",
        "    print(\"Lütfen önceki hücrelerin doğru çalıştığından emin olun.\")\n",
        "    bert_test_metrics = None"
      ],
      "metadata": {
        "id": "uihw1deYNy7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hücre 9: Karmaşıklık Matrisi ve ROC Eğrisinin Çizdirilmesi (Test Seti Üzerinden)\n",
        "\n",
        "* **Amaç:**\n",
        "    Modelin test seti üzerindeki sınıflandırma performansını Karmaşıklık Matrisi ve ROC Eğrisi ile görsel olarak analiz etmek.\n",
        "\n",
        "* **Yapılan İşlemler:**\n",
        "    1.  `trainer.predict(tokenized_datasets[\"test\"])` ile test seti üzerindeki ham tahminler (logitler) ve gerçek etiketler alınır.\n",
        "    2.  Logitler, `torch.softmax` ile olasılıklara dönüştürülür. `np.argmax` ile en yüksek olasılıklı sınıf, tahmin edilen etiket (`predicted_labels`) olarak belirlenir.\n",
        "    3.  **Karmaşıklık Matrisi:** `sklearn.metrics.confusion_matrix` ile hesaplanır ve `seaborn.heatmap` ile görselleştirilir. TN, FP, FN, TP değerleri de metin olarak yazdırılır. `label_feature` (Hücre 3'ten) kullanılarak eksen etiketleri \"neg\" ve \"pos\" olarak ayarlanır.\n",
        "    4.  **ROC Eğrisi:** Pozitif sınıfa (etiket 1, \"pos\") ait olasılıklar kullanılır. `sklearn.metrics.roc_curve` ile FPR ve TPR değerleri, `sklearn.metrics.auc` ile de AUC değeri hesaplanır. `matplotlib.pyplot` ile ROC eğrisi ve rastgele tahmin çizgisi çizdirilir, AUC değeri grafiğe eklenir.\n",
        "    5.  Bu hücrede hesaplanan AUC ile Hücre 8'de `trainer.evaluate` ile elde edilen AUC karşılaştırılır.\n",
        "\n",
        "* **Uygulama Detayları/Sonuçlar:**\n",
        "    * Renkli ve açıklamalı bir Karmaşıklık Matrisi grafiği çizilir. En son başarılı denemedeki değerler (TN: 11483, FP: 1017, FN: 953, TP: 11547) modelin performansını detaylandırır.\n",
        "    * ROC Eğrisi grafiği çizilir. AUC değerinin 1'e yakın olması (en son 0.9751) modelin iyi bir ayırt ediciliğe sahip olduğunu gösterir.\n",
        "    * İki farklı yöntemle hesaplanan AUC değerlerinin tutarlı olması (0.9751) beklenir ve doğrulanır."
      ],
      "metadata": {
        "id": "UFEGpoxZN3ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 9: Karmaşıklık Matrisi ve ROC Eğrisinin Çizdirilmesi (Test Seti Üzerinden)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, roc_curve\n",
        "from sklearn.metrics import auc as sklearn_auc # sklearn.metrics.auc'yi import ediyoruz\n",
        "# numpy ve torch zaten import edilmişti\n",
        "\n",
        "# Gerekli değişkenlerin varlığını ve doğruluğunu kontrol edelim\n",
        "if 'trainer' in globals() and trainer is not None and \\\n",
        "   'tokenized_datasets' in globals() and \"test\" in tokenized_datasets and \\\n",
        "   'bert_test_metrics' in globals() and bert_test_metrics is not None and \\\n",
        "   'label_feature' in globals() and label_feature is not None and hasattr(label_feature, 'names') and hasattr(label_feature, 'int2str'):\n",
        "\n",
        "    print(\"Test seti üzerinde tahminler alınıyor...\")\n",
        "    try:\n",
        "        test_predictions_output = trainer.predict(tokenized_datasets[\"test\"])\n",
        "\n",
        "        logits = test_predictions_output.predictions\n",
        "        probabilities = torch.softmax(torch.tensor(logits), dim=-1).cpu().numpy()\n",
        "        predicted_labels = np.argmax(logits, axis=1)\n",
        "        true_labels = test_predictions_output.label_ids\n",
        "\n",
        "        # 1. Karmaşıklık Matrisi (Confusion Matrix)\n",
        "        # Etiketlerin [0, 1] olduğunu ve label_feature.names'in ['neg', 'pos'] olduğunu varsayıyoruz\n",
        "        cm_labels_numeric = [0, 1] # Sayısal etiketler\n",
        "        cm_display_labels = label_feature.names # Görüntülenecek etiketler\n",
        "\n",
        "        cm = confusion_matrix(true_labels, predicted_labels, labels=cm_labels_numeric)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                    xticklabels=cm_display_labels,\n",
        "                    yticklabels=cm_display_labels)\n",
        "        plt.title('Karmaşıklık Matrisi (Test Seti)')\n",
        "        plt.xlabel('Tahmin Edilen Etiket')\n",
        "        plt.ylabel('Gerçek Etiket')\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nKarmaşıklık Matrisi Değerleri:\")\n",
        "        # cm[0,0]=TN (neg-neg), cm[0,1]=FP (neg-pos), cm[1,0]=FN (pos-neg), cm[1,1]=TP (pos-pos)\n",
        "        print(f\"  Doğru Negatif (TN) [{cm_display_labels[0]}-{cm_display_labels[0]}]: {cm[0,0]}\")\n",
        "        print(f\"  Yanlış Pozitif (FP) [{cm_display_labels[0]}-{cm_display_labels[1]}]: {cm[0,1]}\")\n",
        "        print(f\"  Yanlış Negatif (FN) [{cm_display_labels[1]}-{cm_display_labels[0]}]: {cm[1,0]}\")\n",
        "        print(f\"  Doğru Pozitif (TP) [{cm_display_labels[1]}-{cm_display_labels[1]}]: {cm[1,1]}\")\n",
        "\n",
        "        # 2. ROC Eğrisi\n",
        "        # Pozitif sınıfın ID'sini alalım (genellikle 1)\n",
        "        positive_class_id = label_feature.str2int('pos') if hasattr(label_feature, 'str2int') else 1\n",
        "\n",
        "        # Sadece pozitif sınıfın olasılıklarını alalım\n",
        "        positive_class_probabilities = probabilities[:, positive_class_id]\n",
        "\n",
        "        fpr, tpr, thresholds = roc_curve(true_labels, positive_class_probabilities, pos_label=positive_class_id)\n",
        "        roc_auc_value_sklearn = sklearn_auc(fpr, tpr)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC eğrisi (AUC = {roc_auc_value_sklearn:.4f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') # Rastgele tahmin çizgisi\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('Yanlış Pozitif Oranı (1 - Specificity)')\n",
        "        plt.ylabel('Doğru Pozitif Oranı (Recall/Sensitivity)')\n",
        "        plt.title('Alıcı İşletim Karakteristiği (ROC) Eğrisi')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"\\nHücre 8'de hesaplanan Test AUC (trainer.evaluate): {bert_test_metrics.get('eval_auc', 'N/A'):.4f}\")\n",
        "        print(f\"Bu hücrede ROC eğrisi için hesaplanan AUC (sklearn): {roc_auc_value_sklearn:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTahminler alınırken veya görselleştirmeler oluşturulurken bir hata oluştu: {e}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "else:\n",
        "    print(\"Hata: Gerekli değişkenler ('trainer', 'tokenized_datasets['test']', 'bert_test_metrics', 'label_feature') bulunamadı veya 'label_feature' doğru yapılandırılmamış.\")\n",
        "    print(\"Lütfen önceki hücrelerin doğru çalıştığından emin olun.\")"
      ],
      "metadata": {
        "id": "HDxNgzzYN2jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hücre 10: Eğitim ve Doğrulama Kayıp/Metrik Grafiğinin Çizdirilmesi (Tam Veriyle)\n",
        "\n",
        "* **Amaç:**\n",
        "    Modelin eğitim süreci boyunca performansının nasıl değiştiğini (kayıp ve doğruluk değerleri açısından) hem eğitim hem de doğrulama veri setleri üzerinde epoch bazında görselleştirmek. Bu grafikler, modelin öğrenme eğrisini, genelleme yeteneğini ve aşırı öğrenme (overfitting) gibi durumları net bir şekilde analiz etmeye yardımcı olur.\n",
        "\n",
        "* **Yapılan İşlemler:**\n",
        "    1.  `Trainer` objesinin eğitim sırasında kaydettiği `trainer.state.log_history` alınır. Güncellenmiş `TrainingArguments` sayesinde bu geçmiş, her epoch sonunda hem eğitim kaybını (`loss`) hem de doğrulama metriklerini (`eval_loss`, `eval_accuracy` vb.) içeren sözlükler listesidir.\n",
        "    2.  `log_history` bir `pandas.DataFrame`'e dönüştürülerek işlenmesi kolaylaştırılır.\n",
        "    3.  DataFrame üzerinden, eğitim logları (sadece `loss` içeren ve `eval_loss` içermeyenler) ve doğrulama logları (`eval_loss` ve diğer `eval_` metriklerini içerenler) ayrı ayrı filtrelenir.\n",
        "    4.  `matplotlib.pyplot` kullanılarak:\n",
        "        * **Birinci Grafik:** Epoch bazında hem \"Eğitim Kaybı\" hem de \"Doğrulama Kaybı\" aynı eksen üzerinde çizdirilir.\n",
        "        * **İkinci Grafik:** Epoch bazında \"Doğrulama Doğruluğu\" (`eval_accuracy`) çizdirilir. (İsteğe bağlı olarak diğer doğrulama metrikleri de (F1, Precision, Recall) benzer şekilde çizdirilebilir.)\n",
        "    5.  Grafiklerin başlıkları, eksen etiketleri ve lejantları (açıklama yazıları) eklenir. X ekseni epoch sayılarını gösterecek şekilde net olarak ayarlanır.\n",
        "\n",
        "* **Uygulama Detayları/Sonuçlar (Beklenen):**\n",
        "    * `log_history`'nin ham içeriğinden örnekler yazdırılır; bu, her epoch için hem eğitim hem de doğrulama kayıtlarının varlığını teyit eder.\n",
        "    * \"Eğitim ve Doğrulama Kaybı\" grafiği:\n",
        "        * Eğitim kaybının (mavi çizgi) epoch'lar boyunca sürekli düşmesi beklenir.\n",
        "        * Doğrulama kaybının (kırmızı çizgi) başlangıçta düşmesi, ancak bir noktadan sonra (aşırı öğrenme başlarsa) artmaya başlaması veya durağanlaşması gözlemlenebilir. En son başarılı çalıştırmanızdaki loglara göre, doğrulama kaybı 2. epoch'ta minimuma ulaşıp 3. epoch'ta artmıştır, bu da aşırı öğrenmeyi gösterir.\n",
        "    * \"Doğrulama Doğruluğu\" grafiği:\n",
        "        * Doğrulama doğruluğunun epoch'lar boyunca değişimi gözlemlenir. Genellikle doğrulama kaybının minimum olduğu epoch civarında maksimuma ulaşır veya durağanlaşır.\n",
        "    * Bu grafikler, model için en uygun epoch sayısını belirlemede ve `load_best_model_at_end=True` ayarının hangi checkpoint'i seçtiğini anlamada kritik öneme sahiptir."
      ],
      "metadata": {
        "id": "x5tc3IkROAac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 10 (Geliştirilmiş Log İşleme ve Tam Veriyle Grafikler):\n",
        "# Eğitim ve Doğrulama Kayıp/Metrik Grafikleri\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# trainer ve log_history'nin Hücre 7'den geldiğini varsayıyoruz\n",
        "if 'trainer' in globals() and trainer is not None and \\\n",
        "   hasattr(trainer.state, 'log_history') and trainer.state.log_history:\n",
        "\n",
        "    log_history = trainer.state.log_history\n",
        "\n",
        "    print(\"Ham log_history içeriği (ilk 3 ve son 3 kayıt - eğer yeterliyse):\")\n",
        "    # log_history'nin yapısını daha iyi anlamak için birkaç giriş yazdıralım\n",
        "    num_entries_to_show = 3\n",
        "    if len(log_history) > 2 * num_entries_to_show:\n",
        "        for i in range(num_entries_to_show): print(log_history[i])\n",
        "        print(\"...\")\n",
        "        for i in range(len(log_history) - num_entries_to_show, len(log_history)): print(log_history[i])\n",
        "    else:\n",
        "        for entry in log_history: print(entry)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # log_history'yi bir DataFrame'e dönüştürelim\n",
        "    log_df = pd.DataFrame(log_history)\n",
        "\n",
        "    # Eğitim logları: 'loss' anahtarını içerir ve 'eval_loss' anahtarını içermez\n",
        "    # Ayrıca, 'learning_rate' içerenler genellikle eğitim adımı loglarıdır.\n",
        "    train_logs_df = log_df[log_df['loss'].notna() & log_df['eval_loss'].isna()].copy()\n",
        "\n",
        "    # Değerlendirme logları: 'eval_loss' anahtarını içerir\n",
        "    eval_logs_df = log_df[log_df['eval_loss'].notna()].copy()\n",
        "\n",
        "    # Grafikleri çizdirelim\n",
        "    if not train_logs_df.empty and not eval_logs_df.empty:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Eğitim Kaybı\n",
        "        plt.plot(train_logs_df['epoch'], train_logs_df['loss'], 'b-o', label='Eğitim Kaybı (Training Loss)')\n",
        "\n",
        "        # Doğrulama Kaybı\n",
        "        plt.plot(eval_logs_df['epoch'], eval_logs_df['eval_loss'], 'r-s', label='Doğrulama Kaybı (Validation Loss)')\n",
        "\n",
        "        plt.title('Epoch Bazında Eğitim ve Doğrulama Kaybı')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Kayıp (Loss)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # X ekseni için epoch değerlerini belirle\n",
        "        # Epochlar genellikle tam sayılar (1.0, 2.0, 3.0 ...) veya bunlara çok yakın ondalıklar olur.\n",
        "        # En son log_history'nizde epochlar tam sayıydı.\n",
        "        epochs_present = sorted(list(set(train_logs_df['epoch'].round().tolist() + eval_logs_df['epoch'].round().tolist())))\n",
        "        if epochs_present:\n",
        "            min_e, max_e = min(epochs_present), max(epochs_present)\n",
        "            plt.xticks(np.arange(int(min_e), int(max_e) + 1, 1.0))\n",
        "            plt.xlim(left=int(min_e) - 0.5, right=int(max_e) + 0.5) # Grafiğin kenarlarında biraz boşluk bırak\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        # Doğrulama doğruluğu grafiği\n",
        "        if 'eval_accuracy' in eval_logs_df.columns and not eval_logs_df['eval_accuracy'].isna().all():\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            plt.plot(eval_logs_df['epoch'], eval_logs_df['eval_accuracy'], 'g-^', label='Doğrulama Doğruluğu (Validation Accuracy)')\n",
        "            plt.title('Epoch Bazında Doğrulama Doğruluğu')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Doğruluk (Accuracy)')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            if epochs_present: # Aynı x-ekseni ayarını kullanalım\n",
        "                plt.xticks(np.arange(int(min_e), int(max_e) + 1, 1.0))\n",
        "                plt.xlim(left=int(min_e) - 0.5, right=int(max_e) + 0.5)\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"\\nDoğrulama doğruluğu ('eval_accuracy') loglarda anlamlı şekilde bulunamadı veya tüm değerler NaN.\")\n",
        "            print(\"Mevcut doğrulama logları (epoch ve eval_accuracy sütunları):\")\n",
        "            if 'epoch' in eval_logs_df and 'eval_accuracy' in eval_logs_df:\n",
        "                print(eval_logs_df[['epoch', 'eval_accuracy']].to_string())\n",
        "            elif 'epoch' in eval_logs_df:\n",
        "                 print(eval_logs_df[['epoch']].to_string())\n",
        "            else:\n",
        "                print(eval_logs_df.to_string())\n",
        "\n",
        "    elif train_logs_df.empty:\n",
        "        print(\"Eğitim logları ('loss' içeren ve 'eval_loss' içermeyen) filtrelenemedi. Ham log_df:\")\n",
        "        print(log_df.to_string())\n",
        "    elif eval_logs_df.empty:\n",
        "        print(\"Doğrulama logları ('eval_loss' içeren) filtrelenemedi. Ham log_df:\")\n",
        "        print(log_df.to_string())\n",
        "    else:\n",
        "        print(\"Grafik çizdirmek için yeterli eğitim veya doğrulama logu bulunamadı.\")\n",
        "\n",
        "else:\n",
        "    print(\"Hata: 'trainer' objesi veya 'log_history' bulunamadı.\")\n",
        "    print(\"Lütfen önceki hücrelerin (özellikle Hücre 7 - eğitim) doğru çalıştığından emin olun.\")"
      ],
      "metadata": {
        "id": "TvOfvZv3OB8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hücre 11: Eğitim ve Çıkarım Sürelerinin Hesaplanması\n",
        "\n",
        "* **Amaç:**\n",
        "    Modelin hem eğitim (fine-tuning) süresini hem de eğitilmiş modelle yeni veriler üzerinde tahmin yapma (çıkarım/inference) hızını belirlemek.\n",
        "\n",
        "* **Yapılan İşlemler:**\n",
        "    1.  **Eğitim Süresi:** Hücre 7'de model eğitimi sırasında ölçülen ve `training_time` değişkeninde saklanan (veya `log_history`'deki `train_runtime`'dan alınan) toplam eğitim süresi saniye ve dakika cinsinden yazdırılır.\n",
        "    2.  **Örnek Başına Ortalama Çıkarım Süresi:**\n",
        "        * Model (`model` - en iyi checkpoint `load_best_model_at_end=True` ile yüklendiği varsayılır) değerlendirme moduna (`model.eval()`) alınır.\n",
        "        * Tokenize edilmiş test seti (`tokenized_datasets[\"test\"]`) kullanılır. Çıkarım için sadece modelin ihtiyaç duyduğu sütunlar (`input_ids`, `attention_mask`, `token_type_ids`) seçilir.\n",
        "        * Test verileri, `torch.utils.data.DataLoader` ile yığınlar (batch) halinde işlenmek üzere hazırlanır.\n",
        "        * `torch.no_grad()` bloğu içinde, her bir yığın için modelden tahminler alınır ve yığının işlenme süresi ölçülür.\n",
        "        * Toplam çıkarım süresi ve işlenen toplam örnek sayısı kullanılarak örnek başına ortalama çıkarım süresi ve saniyede işlenen örnek sayısı hesaplanıp yazdırılır.\n",
        "\n",
        "* **Uygulama Detayları/Sonuçlar:**\n",
        "    * Toplam eğitim süresi yazdırılır (en son başarılı denemede ~22.12 dakika idi).\n",
        "    * Test seti üzerinde çıkarım yapılırken ilerleme çubuğu gösterilir.\n",
        "    * Çıkarım tamamlandığında, toplam çıkarım süresi, örnek başına ortalama çıkarım süresi (en son denemede ~0.3 milisaniye/örnek) ve saniyede işlenen örnek sayısı (en son denemede ~3326 örnek/saniye) ekrana yazdırılır."
      ],
      "metadata": {
        "id": "ajwaJr5gOHaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hücre 11: Eğitim ve Çıkarım Sürelerinin Hesaplanması\n",
        "\n",
        "# time, torch, tqdm.auto, DataLoader zaten import edilmiş olmalı\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Eğitim Süresi\n",
        "calculated_training_time_value = None\n",
        "if 'training_time' in globals() and training_time is not None: # Hücre 7'de global yapılmıştı\n",
        "    calculated_training_time_value = training_time\n",
        "elif 'trainer' in globals() and trainer is not None and \\\n",
        "     hasattr(trainer.state, 'log_history') and trainer.state.log_history:\n",
        "    for log_entry in reversed(trainer.state.log_history):\n",
        "        if 'train_runtime' in log_entry: # Trainer'ın logladığı genel eğitim süresi\n",
        "            calculated_training_time_value = log_entry['train_runtime']\n",
        "            break\n",
        "\n",
        "if calculated_training_time_value is not None:\n",
        "    print(f\"Toplam Model Eğitim Süresi: {calculated_training_time_value:.2f} saniye ({calculated_training_time_value/60:.2f} dakika)\")\n",
        "else:\n",
        "    print(\"Eğitim süresi bilgisi `training_time` değişkeninden veya `log_history`'den alınamadı.\")\n",
        "\n",
        "# 2. Örnek Başına Ortalama Çıkarım Süresi (Test Seti Üzerinden)\n",
        "avg_inference_time_per_sample = None\n",
        "# Gerekli değişkenlerin varlığını kontrol edelim\n",
        "if 'model' in globals() and model is not None and \\\n",
        "   'tokenized_datasets' in globals() and \"test\" in tokenized_datasets and \\\n",
        "   'device' in globals() and 'tokenizer' in globals() and tokenizer is not None :\n",
        "\n",
        "    model.eval() # Modeli değerlendirme moduna al (dropout vb. katmanları etkisizleştirir)\n",
        "\n",
        "    # Modelin beklediği giriş sütunlarını belirle\n",
        "    if hasattr(tokenizer, 'model_input_names'):\n",
        "        model_input_names = tokenizer.model_input_names\n",
        "    else:\n",
        "        model_input_names = ['input_ids', 'attention_mask', 'token_type_ids']\n",
        "        if 'token_type_ids' not in tokenized_datasets[\"test\"].features: # Eğer veri setinde yoksa listeden çıkar\n",
        "            if 'token_type_ids' in model_input_names: model_input_names.remove('token_type_ids')\n",
        "\n",
        "    try:\n",
        "        # Sadece modelin ihtiyaç duyduğu sütunları içeren bir test seti kopyası oluştur\n",
        "        # ve formatını torch olarak ayarla\n",
        "        inference_ready_dataset = tokenized_datasets[\"test\"].select_columns(model_input_names)\n",
        "        inference_ready_dataset.set_format(\"torch\")\n",
        "    except Exception as e_cols: # select_columns yoksa veya başka bir hata olursa\n",
        "        print(f\"Uyarı: Çıkarım için sütun seçimi/formatlamada sorun ({e_cols}). Eski yöntem denenecek.\")\n",
        "        try:\n",
        "            current_test_set_columns = tokenized_datasets[\"test\"].column_names\n",
        "            cols_to_remove_for_inference = [col for col in current_test_set_columns if col not in model_input_names]\n",
        "            inference_ready_dataset = tokenized_datasets[\"test\"].remove_columns(cols_to_remove_for_inference)\n",
        "            inference_ready_dataset.set_format(\"torch\")\n",
        "        except Exception as e_format_fallback:\n",
        "            print(f\"Formatlama için fallback de başarısız: {e_format_fallback}. Orijinal test seti kullanılacak.\")\n",
        "            inference_ready_dataset = tokenized_datasets[\"test\"] # Son çare\n",
        "\n",
        "\n",
        "    inference_batch_size = 32 # Eğitimdeki eval_batch_size ile aynı olabilir\n",
        "    inference_dataloader = DataLoader(inference_ready_dataset, batch_size=inference_batch_size)\n",
        "\n",
        "    total_inference_time_val = 0.0 # İsim çakışmasını önlemek için farklı bir isim\n",
        "    num_samples_processed = 0\n",
        "\n",
        "    print(f\"\\nTest seti üzerinde çıkarım süresi hesaplanıyor (Batch Size: {inference_batch_size})...\")\n",
        "\n",
        "    with torch.no_grad(): # Gradyan hesaplamalarını kapat\n",
        "        for batch in tqdm(inference_dataloader, desc=\"Çıkarım Yapılıyor\"):\n",
        "            inputs_for_model = {}\n",
        "            valid_batch = True\n",
        "            for name in model_input_names:\n",
        "                if name in batch:\n",
        "                    inputs_for_model[name] = batch[name].to(device)\n",
        "                else: # Beklenen girdi batch'te yoksa\n",
        "                    # print(f\"Uyarı: Beklenen girdi '{name}' batch'te bulunamadı.\")\n",
        "                    # Bu durum genellikle collate_fn veya dataset formatıyla ilgilidir.\n",
        "                    # Eğer input_ids yoksa, bu batch'i atlamak daha güvenli olabilir.\n",
        "                    if name == 'input_ids': valid_batch = False\n",
        "                    break\n",
        "\n",
        "            if not valid_batch or not inputs_for_model :\n",
        "                try: num_in_batch = len(batch.get('input_ids', [])) # input_ids varsa onunla say\n",
        "                except: num_in_batch = 0\n",
        "                num_samples_processed += num_in_batch # Atlananları da sayıma ekleyelim (ya da işlemeyelim)\n",
        "                                                    # Şimdilik, eğer input_ids yoksa bu batch'i sayıma eklemeyelim.\n",
        "                if num_in_batch > 0 and not valid_batch : print(f\"Uyarı: Geçerli model girdisi eksik, {num_in_batch} örnek içeren bu yığın atlandı.\")\n",
        "                continue\n",
        "\n",
        "            current_batch_size = len(inputs_for_model['input_ids'])\n",
        "            num_samples_processed += current_batch_size\n",
        "\n",
        "            start_batch_time = time.perf_counter()\n",
        "            outputs = model(**inputs_for_model)\n",
        "            end_batch_time = time.perf_counter()\n",
        "\n",
        "            batch_time = end_batch_time - start_batch_time\n",
        "            total_inference_time_val += batch_time\n",
        "\n",
        "    if num_samples_processed > 0 and total_inference_time_val > 0 :\n",
        "        avg_inference_time_per_sample = total_inference_time_val / num_samples_processed\n",
        "        samples_per_second_inference = num_samples_processed / total_inference_time_val\n",
        "        print(f\"\\nToplam {num_samples_processed} örnek için çıkarım süresi: {total_inference_time_val:.4f} saniye\")\n",
        "        print(f\"Örnek başına ortalama çıkarım süresi: {avg_inference_time_per_sample:.6f} saniye/örnek\")\n",
        "        print(f\"Saniyede işlenen örnek sayısı (çıkarım): {samples_per_second_inference:.2f} örnek/saniye\")\n",
        "    elif num_samples_processed > 0 and total_inference_time_val == 0 :\n",
        "        print(f\"\\nToplam {num_samples_processed} örnek için çıkarım süresi çok kısa (< ölçülebilir hassasiyet), hız çok yüksek.\")\n",
        "        avg_inference_time_per_sample = 0.0\n",
        "    else:\n",
        "        print(\"Çıkarım için hiç örnek işlenemedi veya süre ölçülemedi.\")\n",
        "\n",
        "else:\n",
        "    print(\"Hata: Model, tokenize edilmiş test seti, tokenizer veya cihaz bilgisi bulunamadı.\")"
      ],
      "metadata": {
        "id": "npdqdHKZOJ5G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}